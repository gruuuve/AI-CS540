at
first
the
report
stated
the
research
trend
.
in
the
computer
vision
community
,
convolutional
neural
network
has
boosted
the
performance
of
the
accuracy
of
image
classification
tasks
.
for
example
,
state
of
the
art
method
-lrb-
1
-rrb-
is
able
to
achieve
84.4
top
1
accuracy
on
imagenet
.
after
the
emergence
of
resnet
-lrb-
2
-rrb-
,
which
allows
cnn
with
larger
number
of
layers
by
adding
residual
blocks
to
be
possible
for
training
,
was
proposed
,
the
problem
of
overfitting
training
data
was
alleviated
from
there
on
.
the
performance
of
cnn
seems
promising
right
now
.
as
the
report
later
stated
the
application
of
self-driving
car
,
current
cnn
provides
the
performance
that
is
capable
of
automated
driving
and
improves
computers
'
ability
of
perception
.
the
report
also
indicates
that
self-driving
car
will
be
widely
adopted
in
2020
.
everything
seems
to
be
promising
.
however
,
i
consider
achieving
this
goal
is
harder
than
what
the
article
expected
.
as
opposite
to
the
advantages
stated
in
report
,
today
's
deep
learning
methods
in
computer
vision
are
not
explainable
in
terms
of
what
information
it
learns
and
are
not
robust
and
stable
enough
.
from
my
opinion
,
even
though
today
's
deep
learning
methods
are
able
to
achieve
performance
that
is
comparable
to
human
,
they
still
have
flaw
.
first
of
all
,
as
i
surveyed
many
deep
learning
methods
in
my
field
,
i
feel
that
many
times
those
work
in
literature
are
not
sure
about
what
a
deep
neural
network
learned
during
training
,
nor
do
they
explain
this
.
interpreting
the
problem
of
what
the
network
learned
from
the
training
data
is
almost
impossible
.
for
example
,
the
resnet-152
-lrb-
2
-rrb-
model
has
nearly
60,344,232
total
number
of
parameters
.
in
this
case
the
network
is
a
black
box
since
it
is
extremely
hard
to
decode
information
from
such
huge
number
and
therefore
interpret
how
such
a
black
box
works
.
i
consider
using
an
explainable
and
reliable
method
is
important
,
especially
in
applications
such
as
self-driving
car
.
in
autonomous
driving
application
,
we
can
not
allow
too
much
fault
tolerance
.
safety
is
always
the
priority
.
sensing
algorithms
relying
on
deep
learning
methods
might
be
problematic
.
if
we
do
not
know
what
information
it
learns
,
its
behavior
might
be
undefined
while
encountering
rare
circumstances
.
also
,
current
deep
learning
methods
are
vulnerable
to
adversarial
samples
.
for
example
,
by
changing
the
texture
of
elephant
in
an
image
,
such
adding
water
ripple
effects
on
it
,
a
deep
neural
network
might
be
tricked
and
can
not
recognize
it
,
even
though
the
image
still
looks
completely
normal
for
a
human
-lrb-
3
-rrb-
.
this
is
the
reason
why
researchers
today
are
still
working
on
adversarial
machine
learning
.
as
the
report
stated
,
this
is
the
first
time
that
computer
is
capable
to
process
visual
classification
task
better
than
human
with
the
help
of
deep
learning
and
large-scale
training
.
this
is
true
while
talking
about
benchmark
on
datasets
such
as
imagenet
.
it
is
hard
to
deny
the
performance
of
deep
learning
methods
stated
in
the
report
.
however
,
at
the
same
time
these
neural
networks
are
not
stable
nor
robust
in
rare
circumstances
.
another
example
of
unexplainable
deep
learning
method
is
that
,
for
3d
human
pose
estimation
applications
,
the
essence
of
many
methods
that
use
monocular
rgb
camera
was
to
find
out
where
the
joint
of
human
body
is
likely
to
occur
,
without
any
geometric
constrain
.
references
-lrb-
1
-rrb-
``
-lrb-
1905.11946
-rrb-
efficientnet
:
rethinking
model
scaling
for
convolutional
neural
networks
.
''
-lrb-
online
-rrb-
.
available
:
https://arxiv.org/abs/1905.11946
.
-lrb-
accessed
:
22-jan-2020
-rrb-
.
-lrb-
2
-rrb-
``
-lrb-
1512.03385
-rrb-
deep
residual
learning
for
image
recognition
.
''
-lrb-
online
-rrb-
.
available
:
https://arxiv.org/abs/1512.03385
.
-lrb-
accessed
:
25-jan-2020
-rrb-
.
-lrb-
3
-rrb-
x.
yuan
,
p.
he
,
q.
zhu
,
and
x.
li
,
``
adversarial
examples
:
attacks
and
defenses
for
deep
learning
,
''
arxiv171207107
cs
stat
,
jul.
2018
.
