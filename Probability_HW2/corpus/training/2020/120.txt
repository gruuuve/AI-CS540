ai
100
the
one
hundred
year
study
on
artificial
intelligence
investigated
the
field
of
ai
and
the
effects
it
has
on
humanity
and
society
.
as
of
2015
when
the
writing
was
published
,
artificial
intelligence
has
evolved
into
a
complex
field
with
many
subcategories
:
large-scale
machine
learning
,
deep
learning
,
reinforcement
learning
,
robotics
,
computer
vision
,
natural
language
processing
,
iot
,
neuromorphic
computing
,
crowdsourcing
,
and
more
.
our
society
has
been
profoundly
changed
by
this
emerging
technology
and
questions
should
be
raised
along
the
process
to
reinforce
the
beneficial
outcomes
and
minimize
the
negative
effects
.
according
to
nils
j.
nilsson
,
artificial
intelligence
is
the
quality
of
a
machine
to
function
appropriately
and
to
react
to
the
environment
.
this
definition
is
very
broad
and
in
practice
,
people
tend
to
compare
a
machine
's
problem-solving
ability
to
that
of
human
intelligence
.
using
the
human
brain
as
a
benchmark
raises
the
question
of
what
is
our
relationship
with
ai
loaded
machines
when
their
so-called
intelligence
matches
or
even
surpasses
ours
?
this
is
no
science
fiction
.
natural
language
processing
has
become
so
advanced
that
translating
an
article
on
google
translate
takes
only
a
few
seconds
and
the
result
is
astoundingly
accurate
.
voice
assistants
can
recognize
and
respond
to
questions
in
realtime
,
even
with
a
good
sense
of
humor
.
the
debate
here
is
often
about
whether
machines
``
understand
''
a
human
language
or
is
just
playing
by
a
set
of
very
complex
rules
-lrb-
as
in
the
chinese
room
argument
-rrb-
.
and
if
science
can
prove
that
those
are
the
same
thing
,
that
what
we
define
as
understanding
a
language
is
merely
the
human
brain
playing
by
the
rules
,
how
do
we
claim
,
with
certainty
,
that
computers
hold
the
same
rule
book
as
we
do
?
from
undergraduate
research
experience
,
i
learned
about
the
operation
of
embedding
,
taking
a
corpus
and
converting
it
to
a
vector
space
to
find
the
correlations
between
word
pairs
.
some
word
pairs
make
sense
to
us
when
we
read
it
,
others
are
not
so
much
.
for
each
dataset
,
there
can
be
multiple
embeddings
,
meaning
that
the
corpus
can
be
interpreted
by
the
algorithms
in
different
ways
.
so
if
we
rapidly
adopt
the
machine
's
understandings
and
interpretations
of
our
language
for
applications
like
translation
,
would
that
profoundly
shift
the
correlations
between
words
without
us
even
noticing
?
another
area
that
is
worth
putting
a
substantial
amount
of
consideration
is
self-driving
car
technology
.
i
have
faith
in
the
tech
that
ai
will
one
day
become
better
drivers
than
human
beings
.
however
,
as
long
as
it
can
not
-lrb-
and
probably
will
not
-rrb-
reduce
the
number
of
accidents
to
zero
,
there
's
always
this
question
of
who
should
be
responsible
.
should
we
blame
the
driver
,
who
by
the
time
will
be
so
used
to
the
technology
that
may
not
even
know
how
to
drive
a
car
anymore
?
should
we
blame
the
company
who
produced
the
technology
who
operates
tens
of
thousands
of
those
cars
and
already
published
their
study
that
their
algorithms
,
although
not
perfect
,
already
reduced
accident
rates
by
tenfold
.
or
should
we
blame
the
car
for
being
autonomous
?
the
questions
can
not
be
answered
easily
.
