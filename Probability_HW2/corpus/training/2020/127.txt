in
this
paper
,
the
author
identifies
the
responsibility
gap
that
could
exist
as
a
result
of
autonomous
vehicles
.
this
means
that
if
something
were
to
go
wrong
,
we
could
not
accurately
blame
someone
for
unprecedented
mistakes
or
errors
that
may
occur
.
in
the
case
of
self-driving
cars
,
we
would
not
be
able
to
assign
blame
to
one
thing
if
something
went
wrong
.
however
,
this
issue
can
be
extended
to
more
than
just
autonomous
vehicles
and
feels
like
a
bigger
problem
for
widespread
implementation
of
ai
in
general
.
for
example
,
if
ai
is
used
in
the
field
of
healthcare
,
we
could
still
face
a
similar
issue
.
keeping
in
mind
that
these
algorithms
and
models
are
not
always
perfect
and
do
have
some
room
for
error
,
it
isn
t
completely
impossible
to
think
that
an
algorithm
could
give
the
wrong
diagnosis
to
a
patient
.
in
some
cases
,
such
an
error
could
potentially
be
life
threatening
or
seriously
detrimental
to
one
s
health
.
once
again
,
there
is
no
way
for
us
to
assign
blame
for
such
an
error
.
we
could
not
just
blame
the
doctor
because
they
were
relying
on
the
ai
for
diagnosis
.
however
,
we
could
also
not
rely
on
the
ai
because
we
do
not
know
if
the
error
resulted
due
to
poor
design
by
the
programmer
or
if
something
went
wrong
later
.
this
problem
becomes
much
more
obvious
if
ai
is
used
by
machines
while
performing
delicate
procedures
such
as
surgery
.
the
machine
could
either
make
the
slightest
mistake
while
performing
a
delicate
surgery
or
even
make
a
major
mistake
during
a
surgery
which
would
not
have
been
risky
if
a
doctor
performed
it
.
in
such
cases
,
it
would
be
hard
for
us
to
apply
that
blame
to
someone
specific
.
as
a
result
,
we
should
be
more
careful
about
implementing
ai
into
such
sensitive
areas
which
could
potentially
be
life-threatening
.
