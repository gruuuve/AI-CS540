over
the
past
decade
,
the
usage
and
capability
of
ai
has
skyrocketed
.
increasing
computing
power
and
the
advent
of
gpgpu
approaches
to
ai
have
allowed
ai
to
be
used
in
circumstances
that
,
until
very
recently
,
have
only
been
within
the
realm
of
theory
.
we
are
quickly
seeing
the
applications
of
ai
not
be
bottlenecked
by
computational
power
,
but
by
social-cultural
and
political
hurdles
.
many
ai
researchers
and
proponents
predict
a
rapid
transformation
of
the
way
our
society
handles
healthcare
,
elder
care
,
and
education
,
among
many
others
.
however
,
even
if
the
technology
is
on
the
horizon
for
this
transformation
,
transforming
these
fields
requires
a
computer
to
master
a
universal
aspect
of
human
communication
:
empathy
.
since
ai
systems
are
not
human
beings
,
they
may
be
able
to
emulate
empathy
,
but
will
never
be
able
to
truly
understand
the
human
experience
.
additionally
,
the
very
nature
of
ai
being
non-living
and
non-human
prevents
them
from
being
accountable
for
their
actions
.
therefore
,
it
will
be
enormously
challenging
,
if
not
impossible
,
for
the
public
to
truly
embrace
ai
in
fields
such
as
those
mentioned
above
.
some
studies
have
shown
that
people
are
more
willing
to
empathize
with
those
that
are
most
similar
to
themselves
-lrb-
ruef
et
al
.
1997
-rrb-
.
even
the
most
culturally
divergent
humans
are
bound
together
by
some
aspects
of
the
human
experience
,
such
as
the
loss
of
something
close
to
us
or
comforting
someone
in
grief
.
while
the
specifics
of
each
person
's
experience
with
grief\/loss
are
vastly
different
,
most
humans
understand
the
pain
that
accompanies
those
experiences
.
ai
systems
could
not
experience
pain
the
same
way
humans
can
but
could
only
emulate
it
.
for
example
,
most
doctors
will
admit
that
informing
the
families
of
patients
whose
loved
ones
have
passed
is
one
of
the
hardest
parts
of
their
jobs
.
even
for
the
most
seasoned
doctors
,
it
is
a
difficult
and
sometimes
painful
experience
to
break
the
bad
news
to
patients
'
families
.
if
a
robot
,
not
matter
how
convincing
,
delivered
bad
news
to
a
family
,
it
would
appear
cold
and
insensitive
.
ai
has
the
potential
to
revolutionize
how
doctors
make
treatment
decisions
,
but
for
ai
to
replace
doctors
in
situations
where
empathy
is
required
would
take
a
monumental
shift
in
societal
attitudes
towards
ai
.
accountability
in
ai
is
another
hurdle
in
getting
society
to
accept
ai
in
these
sensitive
situations
.
determining
liability
for
an
ai
's
failures
is
difficult
because
the
end
product
of
a
trained
ai
system
is
very
different
than
the
data
used
to
make
the
system
.
for
example
,
holding
the
creators
of
the
dataset
the
ai
was
trained
on
liable
is
problematic
because
during
the
training
process
,
it
is
difficult
to
understand
the
dataset
's
role
on
the
output
of
the
final
system
.
separating
the
influence
of
the
dataset
versus
the
influence
of
the
training
algorithm
is
something
not
fully
understood
,
akin
to
exclusively
blaming
a
problem
child
's
parents
for
his
behavior
.
in
reality
,
the
problem
child
's
behavior
is
likely
a
result
of
multiple
factors
,
not
exclusively
parenting
.
ai
researchers
and
the
courts
will
have
to
create
a
way
of
understanding
the
ai
system
's
intent
,
and
even
then
ai
systems
do
not
have
a
concept
of
right
and
wrong
like
humans
do
.
altogether
,
accountability
of
ai
systems
is
a
dilemma
that
has
very
little
precedence
and
working
through
the
legal
aspects
of
ai
accountability
will
be
a
major
challenge
to
overcome
if
we
let
ai
be
responsible
for
such
sensitive
situations
.
