the
use
of
social
media
has
gone
up
significantly
in
the
past
decade
.
many
use
it
for
most
of
their
current
news
,
education
and
social
outlets
.
to
deal
with
this
growth
,
more
efficient
artificial
intelligence
-lrb-
ai
-rrb-
systems
have
been
implemented
by
large
social
media
companies
.
social
media
ai
has
inadvertently
given
small
creators
with
controversial
opinions
much
more
exposure
than
they
would
otherwise
receive
.
the
one
hundred
year
study
only
touched
briefly
on
the
use
of
ai
in
social
media
,
but
social
media
plays
a
massive
role
in
the
life
of
most
u.s.
citizens
.
these
social
media
giants
like
youtube
,
twitter
and
facebook
should
be
held
responsible
for
their
massive
cultural
influence
.
youtube
s
autoplay
and
video
recommendation
ai
has
been
one
of
the
leading
causes
of
significant
cultural
issues
in
the
past
few
years
.
youtube
s
original
video
recommendation
and
autoplay
algorithm
was
implemented
to
keep
users
on
the
site
for
longer
.
many
people
would
visit
the
site
just
to
watch
a
single
video
and
then
leave
,
but
once
they
implemented
an
autoplay
feature
,
users
would
continue
to
watch
recommended
video
after
recommended
video
.
this
original
system
would
recommend
more
popular
videos
with
a
similar
topic
to
what
was
being
watched
.
this
led
to
the
problem
of
viewers
always
ending
up
on
the
same
incredibly
popular
videos
.
for
example
,
gangnam
style
was
the
most
viewed
video
on
youtube
at
the
time
and
it
would
always
eventually
be
recommended
in
autoplay
.
to
combat
this
homogenization
and
prevent
viewers
from
getting
tired
of
seeing
the
same
popular
videos
too
many
times
,
youtube
changed
the
ai
to
recommend
videos
with
fewer
views
.
this
new
algorithm
had
unintended
consequences
.
viewers
,
after
a
string
of
autoplayed
or
recommended
videos
,
would
frequently
end
up
on
videos
about
the
most
extreme
subset
of
whatever
their
viewing
topic
was
.
these
videos
previously
received
little
exposure
,
but
this
new
algorithm
was
leading
large
numbers
to
them
.
unfortunately
,
many
of
these
niche
videos
were
filled
with
dangerous
conspiracy
theories
,
radical
politics
or
racism
.
this
led
to
the
rapid
spread
of
otherwise
little
seen
topics
,
such
as
flat
earth
theory
,
anti-vax
movements
,
alt-right
conspiracy
theories
and
radicalized
racism
.
for
example
,
if
someone
started
watching
pro-vaccine
videos
,
the
ai
would
eventually
take
them
down
a
hole
of
anti-vax
conspiracy
theory
videos
.
it
took
a
few
years
for
this
problem
to
become
apparent
to
the
larger
internet
community
.
youtube
decided
to
review
its
ai
process
and
has
since
stopped
this
harmful
process
of
leading
only
to
niche
videos
,
but
much
damage
has
already
been
done
.
with
over
a
billion
users
,
many
of
whom
are
young
or
impressionable
,
youtube
should
be
more
careful
with
its
recommendation
ai
.
their
desire
to
keep
users
watching
as
many
videos
as
possible
to
increase
their
advertising
revenue
has
led
to
an
incredibly
harmful
spread
of
misinformation
.
more
people
are
refusing
vaccines
,
the
spread
of
preventable
diseases
has
increased
,
alt-right
terrorists
fighting
against
the
deep
state
have
assaulted
fellow
americans
and
more
people
are
denying
basic
science
-lrb-
like
the
earth
being
round
or
the
existence
of
global
warming
-rrb-
.
these
conspiracy
theorists
have
been
the
result
of
many
breeding
grounds
across
the
internet
,
so
of
course
youtube
does
not
bear
the
full
weight
of
the
spread
of
these
problems
,
but
there
is
no
denying
their
significant
contribution
to
the
spread
of
misinformation
.
