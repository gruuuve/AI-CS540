as
is
natural
with
anything
unfamiliar
,
it
's
understandable
that
some
people
may
have
misgivings
or
concerns
about
the
adoption
and
usage
of
new
technologies
.
for
example
,
in
the
report
artificial
intelligence
and
life
in
2030
,
it
is
mentioned
or
implied
several
times
that
one
of
the
challenges
that
more
widespread
use
of
artificial
intelligence
faces
is
gaining
the
trust
and
confidence
of
the
public
.
however
,
in
the
case
of
ai
,
the
root
of
these
concerns
is
not
so
much
ai
itself
,
but
rather
the
potentially
malicious
applications
of
ai
by
hackers
and
scammers
and
the
risk
imposed
on
those
unfamiliar
with
these
newer
technologies
.
as
such
,
this
responsibility
of
gaining
trust
does
not
fall
to
the
ai
,
but
rather
its
creators
to
ensure
that
their
creations
will
not
be
used
for
any
nefarious
tasks
,
thus
allowing
the
general
public
to
be
more
accepting
of
ai
.
on
its
own
,
ai
is
a
rather
innocuous
invention
.
after
all
,
the
report
notes
on
page
seven
that
``
ai
systems
are
specialized
to
accomplish
particular
tasks
''
.
as
such
,
if
an
ai
is
created
to
be
able
to
recognize
cancer
cells
,
accelerate
a
car
while
making
a
turn
,
or
generate
a
unique
math
problem
to
help
a
developing
child
learn
addition
,
it
will
accomplish
this
task
and
nothing
more
.
the
only
problems
that
can
arise
are
when
harmful
human
intentions
are
added
to
the
ai
,
whether
through
hackers
accessing
and
changing
the
task
that
the
ai
is
intended
to
perform
or
by
inherently
giving
the
ai
a
task
that
will
do
harm
to
people
.
the
ai
performs
whatever
task
it
is
given
without
consideration
for
whether
the
task
is
morally
correct
or
not
,
so
the
ai
itself
can
be
trusted
implicitly
as
it
does
n't
have
the
capacity
for
intentionally
malicious
action
.
instead
,
the
creators
of
the
ai
should
be
the
ones
under
scrutiny
for
either
designing
a
flawed
system
that
a
hacker
can
take
advantage
of
to
cause
harm
,
or
making
an
ai
whose
purpose
it
is
to
commit
crime
,
misconduct
,
or
otherwise
negatively
impact
humans
.
looking
at
an
example
,
one
thing
that
negatively
affects
public
trust
of
ai
is
this
perceived
notion
that
ai
can
impose
upon
people
's
privacy
,
due
to
constant
monitoring
of
human
activity
such
as
when
alexa
,
siri
,
cortana
and
various
other
digital
assistants
listen
to
voice
activity
for
commands
.
yet
the
reason
that
privacy
is
such
a
concern
is
because
of
the
fear
that
other
humans
will
be
able
to
do
something
with
private
information
,
whether
it
be
stealing
credit
card
numbers
or
just
feeling
a
sense
of
disgust
that
a
family
ate
fried
butter
three
meals
in
a
row
.
because
again
,
the
ai
itself
can
not
do
anything
with
private
information
that
it
was
n't
told
to
do
by
its
human
creator
for
these
reasons
,
the
best
way
to
combat
this
supposed
lack
of
public
trust
in
ai
is
ensuring
the
security
of
ai
systems
to
not
change
their
initial
,
innocuous
task
,
educating
people
so
that
they
know
ai
only
performs
the
task
it
's
given
and
nothing
more
,
and
then
of
course
having
ai
creators
keep
their
word
about
not
using
ai
to
harm
,
inconvenience
or
annoy
its
users
.
the
effectiveness
of
such
a
method
can
be
seen
through
car
systems
,
among
other
things
.
ai
like
the
anti-lock
brake
systems
and
traffic
control
systems
do
n't
even
have
the
perceived
public
trust
issue
since
they
're
aptly
named
for
their
narrow
purpose
that
will
only
help
drivers
better
navigate
the
roads
,
and
other
ai
creations
can
learn
from
this
example
.
586
words
